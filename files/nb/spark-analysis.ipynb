{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e4327c1d40d0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f012023fcc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Q1] first we initiate a SparkSession with the spark-master\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1024m\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- total_cases: string (nullable = true)\n",
      " |-- new_cases: string (nullable = true)\n",
      " |-- total_deaths: string (nullable = true)\n",
      " |-- new_deaths: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# [Q2a] load CSV into spark dataframe\n",
    "# and [Q2b] check schema\n",
    "wrong_schema = spark.read.csv(path='../data/covid19.csv',header=True)\n",
    "# by default the type of each column is apparently assumed to be String.\n",
    "print(wrong_schema.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- total_cases: integer (nullable = true)\n",
      " |-- new_cases: integer (nullable = true)\n",
      " |-- total_deaths: integer (nullable = true)\n",
      " |-- new_deaths: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you ask explicitly, spark will try to infer the schema automatically\n",
    "infer_schema = spark.read.csv(path='../data/covid19.csv',header=True,inferSchema=True)\n",
    "infer_schema.printSchema()\n",
    "# in this case it gets the integers right, but just treats the date as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- total_cases: integer (nullable = true)\n",
      " |-- new_cases: integer (nullable = true)\n",
      " |-- total_deaths: integer (nullable = true)\n",
      " |-- new_deaths: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# or you can specify the schema explicitly\n",
    "\n",
    "from pyspark.sql.types import (StructField, \n",
    "                               StringType, \n",
    "                               IntegerType,\n",
    "                               DateType,\n",
    "                               StructType)\n",
    "\n",
    "data_schema = [StructField('continent',StringType(),True),\n",
    "              StructField('location',StringType(),True),\n",
    "              StructField('date',DateType(),True),\n",
    "              StructField('total_cases',IntegerType(),True),\n",
    "              StructField('new_cases',IntegerType(),True),\n",
    "              StructField('total_deaths',IntegerType(),True),\n",
    "              StructField('new_deaths',IntegerType(),True)]\n",
    "\n",
    "correct_struc = StructType(fields=data_schema)\n",
    "\n",
    "dataframe = spark.read.csv(path='../data/covid19.csv', header=True, schema=correct_struc)\n",
    "\n",
    "# and we can confirm that this time the types are correct\n",
    "print(dataframe.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `rdd` <class 'pyspark.rdd.RDD'> from `dataframe` <class 'pyspark.sql.dataframe.DataFrame'>.\n",
      "Created `new_dataframe` <class 'pyspark.sql.dataframe.DataFrame'> from `rdd` <class 'pyspark.rdd.RDD'>.\n"
     ]
    }
   ],
   "source": [
    "# if we wanted to convert to the older-style RDD we easily could\n",
    "rdd = dataframe.rdd\n",
    "print(f'Created `rdd` {type(rdd)} from `dataframe` {type(dataframe)}.')\n",
    "# ... and vice versa\n",
    "new_dataframe = rdd.toDF()\n",
    "print(f'Created `new_dataframe` {type(new_dataframe)} from `rdd` {type(rdd)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the simplest way to drop null values from a spark 2.0 dataframe \n",
    "# ...is like this\n",
    "drop_na = dataframe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Q2c] but we can use the `.filter()` method if we like\n",
    "filtered_df = dataframe.filter(\n",
    "        ' and '.join([f'{x} is not null' for x in dataframe.columns])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering we had 53087 rows...\n",
      "Using `.dropna()` leaves us 39974 rows.\n",
      "Using `.filter()` leaves us 39974 rows.\n",
      "Good, those numbers are the same!\n"
     ]
    }
   ],
   "source": [
    "print(f'Before filtering we had {dataframe.count()} rows...')\n",
    "print(f'Using `.dropna()` leaves us {drop_na.count()} rows.')\n",
    "print(f'Using `.filter()` leaves us {filtered_df.count()} rows.')\n",
    "if drop_na.count() == filtered_df.count():\n",
    "    print('Good, those numbers are the same!')\n",
    "else:\n",
    "    print('Not good -- those numbers should be the same...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Q3] use aggregate and groupBy functions to see highest `total_deaths` in each country\n",
    "hi_total_deaths = filtered_df.groupBy('location').agg({'total_deaths':'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|            location|max(total_deaths)|\n",
      "+--------------------+-----------------+\n",
      "|                Chad|               96|\n",
      "|            Paraguay|             1327|\n",
      "|              Russia|            26589|\n",
      "|               Yemen|              600|\n",
      "|             Senegal|              322|\n",
      "|              Sweden|             5918|\n",
      "|              Guyana|              119|\n",
      "|              Jersey|               32|\n",
      "|         Philippines|             7053|\n",
      "|            Djibouti|               61|\n",
      "|            Malaysia|              238|\n",
      "|           Singapore|               28|\n",
      "|                Fiji|                2|\n",
      "|              Turkey|             9950|\n",
      "|United States Vir...|               21|\n",
      "|      Western Sahara|                1|\n",
      "|              Malawi|              183|\n",
      "|                Iraq|            10724|\n",
      "|Sint Maarten (Dut...|               22|\n",
      "|             Germany|            10183|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hi_total_deaths.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|location|max(total_deaths)|\n",
      "+--------+-----------------+\n",
      "|  Sweden|             5918|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the assignment suggests that the number of total_deaths for Sweden should be 986\n",
    "# however it is actually 5918\n",
    "hi_total_deaths.filter(hi_total_deaths.location=='Sweden').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|location|max(total_deaths)|\n",
      "+--------+-----------------+\n",
      "|  Sweden|              986|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# however, we would get the result 986 if we hadn't explicitly made sure to load the CSV with the correct schema\n",
    "wrong_schema.groupBy('location').agg({'total_deaths':'max'}).filter(wrong_schema.location=='Sweden').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|max(total_cases)|\n",
      "+----------------+\n",
      "|         8779653|\n",
      "+----------------+\n",
      "\n",
      "+---------------------+\n",
      "|min(max(total_cases))|\n",
      "+---------------------+\n",
      "|                   13|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [Q4] use max and min functions to see which country \n",
    "# has highest and lowest `total_cases`\n",
    "# NB: 'total_cases' are given for every date, \n",
    "# so for country with lowest can't simply find min(total_cases)\n",
    "# as we'll get an earlier date with a lower figure\n",
    "# rather than the country with the lowest final total_cases\n",
    "# -- however, this is obviously not an issue for the maximum figure\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "filtered_df.select(F.max('total_cases')).show()\n",
    "filtered_df.groupBy('location').max('total_cases').select(F.min('max(total_cases)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries with Highest Total Number of Cases\n",
      "+--------------+----------------+\n",
      "|      location|max(total_cases)|\n",
      "+--------------+----------------+\n",
      "| United States|         8779653|\n",
      "|         India|         7990322|\n",
      "|        Brazil|         5439641|\n",
      "|        Russia|         1547774|\n",
      "|        France|         1198695|\n",
      "|         Spain|         1116738|\n",
      "|     Argentina|         1116596|\n",
      "|      Colombia|         1033218|\n",
      "|United Kingdom|          917575|\n",
      "|        Mexico|          901268|\n",
      "|          Peru|          892497|\n",
      "|  South Africa|          717851|\n",
      "|          Iran|          581824|\n",
      "|         Italy|          564778|\n",
      "|         Chile|          504525|\n",
      "|       Germany|          464239|\n",
      "|          Iraq|          459908|\n",
      "|    Bangladesh|          401586|\n",
      "|     Indonesia|          396454|\n",
      "|   Philippines|          373144|\n",
      "+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Countries with Lowest Total Number of Cases\n",
      "+--------------------+----------------+\n",
      "|            location|max(total_cases)|\n",
      "+--------------------+----------------+\n",
      "|          Montserrat|              13|\n",
      "|                Fiji|              33|\n",
      "|British Virgin Is...|              72|\n",
      "|Northern Mariana ...|              92|\n",
      "| Antigua and Barbuda|             124|\n",
      "|              Brunei|             148|\n",
      "|Bonaire Sint Eust...|             150|\n",
      "|             Bermuda|             194|\n",
      "|            Barbados|             233|\n",
      "|      Cayman Islands|             239|\n",
      "|            Guernsey|             266|\n",
      "|              Monaco|             320|\n",
      "|         Isle of Man|             352|\n",
      "|           Mauritius|             439|\n",
      "|       Liechtenstein|             483|\n",
      "|            Tanzania|             509|\n",
      "|             Comoros|             517|\n",
      "|              Taiwan|             550|\n",
      "|             Burundi|             558|\n",
      "|              Jersey|             560|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to see a list of the countries with the highest and lowest total_cases count...\n",
    "total_cases = filtered_df.groupBy('location').max('total_cases')\n",
    "print('Countries with Highest Total Number of Cases')\n",
    "total_cases.orderBy('max(total_cases)',ascending=False).show()\n",
    "print('Countries with Lowest Total Number of Cases')\n",
    "total_cases.orderBy('max(total_cases)',ascending=True).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
