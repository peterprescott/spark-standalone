
@article{APerez2020,
  title = {Apache {{Spark Cluster}} on {{Docker}} (Ft. a {{JuyterLab Interface}})},
  author = {Perez, Andr{\'e}},
  year = {2020},
  abstract = {Build your own Apache Spark cluster in standalone mode on Docker with a JupyterLab interface},
  file = {/home/peterprescott/Zotero/storage/MFVNB877/apache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445.html},
  journal = {Towards Data Science},
  language = {en}
}

@misc{BAmen2020,
  title = {{{COMP529}}/336: {{Coursework Assignment}} \#1 ({{Batch Analytics}})},
  author = {Amen, Bakhtiar},
  year = {2020},
  publisher = {{University of Liverpool}}
}

@article{CBoettiger2015,
  title = {An Introduction to {{Docker}} for Reproducible Research},
  author = {Boettiger, Carl},
  year = {2015},
  volume = {49},
  pages = {71--79},
  publisher = {{ACM New York, NY, USA}},
  file = {/home/peterprescott/Zotero/storage/CQ6QN9QE/Boettiger - 2015 - An introduction to Docker for reproducible researc.pdf;/home/peterprescott/Zotero/storage/323LQUY6/2723872.html},
  journal = {ACM SIGOPS Operating Systems Review},
  number = {1}
}

@misc{FPerezGranger2015,
  title = {Project {{Jupyter}}: {{Computational}} Narratives as the Engine of Collaborative Data Science},
  shorttitle = {Project {{Jupyter}}},
  author = {Perez, Fernando and Granger, Brian E.},
  year = {2015},
  file = {/home/peterprescott/Zotero/storage/MLRG3DRB/Perez and Granger - 2015 - Project Jupyter Computational narratives as the e.pdf}
}

@misc{JDamji2016,
  title = {A {{Tale}} of {{Three Apache Spark APIs}}: {{RDDs}} vs {{DataFrames}} and {{Datasets}}},
  shorttitle = {A {{Tale}} of {{Three Apache Spark APIs}}},
  author = {Damji, Jules},
  year = {2016},
  abstract = {In summation, the choice of when to use RDD or DataFrame and/or Dataset seems obvious. While the former offers you low-level functionality and control, the latter allows custom view and structure, offers high-level and domain specific operations, saves space, and executes at superior speeds.},
  file = {/home/peterprescott/Zotero/storage/EMVJPY38/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html},
  journal = {Databricks},
  language = {en-US}
}

@article{JDeanGhemawat2008,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  month = jan,
  volume = {51},
  pages = {107--113},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1327452.1327492},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.},
  file = {/home/peterprescott/Zotero/storage/S4A42LQN/Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {1}
}

@misc{JGrus2018,
  title = {I {{Don}}'t {{Like Notebooks}} - {{Joel Grus}} - \#{{JupyterCon}} 2018},
  author = {Grus, Joel},
  year = {2018},
  abstract = {I Don't Like Notebooks hi, I'm Joel, and I don't like notebooks Joel Grus (@joelgrus) \#JupyterCon 2018},
  file = {/home/peterprescott/Zotero/storage/9BN4W3US/preview.html},
  language = {en}
}

@article{JYuEtAl2019,
  title = {Spatial Data Management in {{Apache Spark}}: The {{GeoSpark}} Perspective and Beyond},
  shorttitle = {Spatial Data Management in Apache Spark},
  author = {Yu, Jia and Zhang, Zongsi and Sarwat, Mohamed},
  year = {2019},
  month = jan,
  volume = {23},
  pages = {37--78},
  issn = {1573-7624},
  doi = {10.1007/s10707-018-0330-9},
  abstract = {The paper presents the details of designing and developing GeoSpark, which extends the core engine of Apache Spark and SparkSQL to support spatial data types, indexes, and geometrical operations at scale. The paper also gives a detailed analysis of the technical challenges and opportunities of extending Apache Spark to support state-of-the-art spatial data partitioning techniques: uniform grid, R-tree, Quad-Tree, and KDB-Tree. The paper also shows how building local spatial indexes, e.g., R-Tree or Quad-Tree, on each Spark data partition can speed up the local computation and hence decrease the overall runtime of the spatial analytics program. Furthermore, the paper introduces a comprehensive experiment analysis that surveys and experimentally evaluates the performance of running de-facto spatial operations like spatial range, spatial K-Nearest Neighbors (KNN), and spatial join queries in the Apache Spark ecosystem. Extensive experiments on real spatial datasets show that GeoSpark achieves up to two orders of magnitude faster run time performance than existing Hadoop-based systems and up to an order of magnitude faster performance than Spark-based systems.},
  file = {/home/peterprescott/Zotero/storage/39PRIG9W/Yu et al. - 2019 - Spatial data management in apache spark the GeoSp.pdf},
  journal = {GeoInformatica},
  language = {en},
  number = {1}
}

@inproceedings{KShvachkoEtAl2010,
  title = {The {{Hadoop Distributed File System}}},
  booktitle = {2010 {{IEEE}} 26th {{Symposium}} on {{Mass Storage Systems}} and {{Technologies}} ({{MSST}})},
  author = {Shvachko, K. and Kuang, H. and Radia, S. and Chansler, R.},
  year = {2010},
  month = may,
  pages = {1--10},
  issn = {2160-1968},
  doi = {10.1109/MSST.2010.5496972},
  abstract = {The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. In a large cluster, thousands of servers both host directly attached storage and execute user application tasks. By distributing storage and computation across many servers, the resource can grow with demand while remaining economical at every size. We describe the architecture of HDFS and report on experience using HDFS to manage 25 petabytes of enterprise data at Yahoo!.},
  file = {/home/peterprescott/Zotero/storage/24XLR87Q/Shvachko et al. - 2010 - The Hadoop Distributed File System.pdf;/home/peterprescott/Zotero/storage/87F7NIRK/5496972.html},
  keywords = {Bandwidth,Clustering algorithms,Computer architecture,Concurrent computing,data storage,data stream,Distributed computing,distributed databases,distributed file system,enterprise data,Facebook,File servers,File systems,Hadoop,Hadoop distributed file system,HDFS,Internet,network operating systems,Protection,Protocols,Yahoo!}
}

@inproceedings{MArmbrustEtAl2015,
  title = {Spark {{SQL}}: {{Relational Data Processing}} in {{Spark}}},
  shorttitle = {Spark {{SQL}}},
  booktitle = {Proceedings of the 2015 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Armbrust, Michael and Xin, Reynold S. and Lian, Cheng and Huai, Yin and Liu, Davies and Bradley, Joseph K. and Meng, Xiangrui and Kaftan, Tomer and Franklin, Michael J. and Ghodsi, Ali and Zaharia, Matei},
  year = {2015},
  month = may,
  pages = {1383--1394},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2723372.2742797},
  abstract = {Spark SQL is a new module in Apache Spark that integrates relational processing with Spark's functional programming API. Built on our experience with Shark, Spark SQL lets Spark programmers leverage the benefits of relational processing (e.g. declarative queries and optimized storage), and lets SQL users call complex analytics libraries in Spark (e.g. machine learning). Compared to previous systems, Spark SQL makes two main additions. First, it offers much tighter integration between relational and procedural processing, through a declarative DataFrame API that integrates with procedural Spark code. Second, it includes a highly extensible optimizer, Catalyst, built using features of the Scala programming language, that makes it easy to add composable rules, control code generation, and define extension points. Using Catalyst, we have built a variety of features (e.g. schema inference for JSON, machine learning types, and query federation to external databases) tailored for the complex needs of modern data analysis. We see Spark SQL as an evolution of both SQL-on-Spark and of Spark itself, offering richer APIs and optimizations while keeping the benefits of the Spark programming model.},
  file = {/home/peterprescott/Zotero/storage/7TYQJH7W/Armbrust et al. - 2015 - Spark SQL Relational Data Processing in Spark.pdf},
  isbn = {978-1-4503-2758-9},
  keywords = {data warehouse,databases,hadoop,machine learning,spark},
  series = {{{SIGMOD}} '15}
}

@inproceedings{MZahariaEtAl2012,
  title = {Resilient {{Distributed Datasets}}: {{A Fault}}-{{Tolerant Abstraction}} for {{In}}-{{Memory Cluster Computing}}},
  shorttitle = {Resilient {{Distributed Datasets}}},
  booktitle = {9th \{\vphantom\}{{USENIX}}\vphantom\{\} {{Symposium}} on {{Networked Systems Design}} and {{Implementation}} (\{\vphantom\}{{NSDI}}\vphantom\{\} 12)},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauly, Murphy and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
  year = {2012},
  pages = {15--28},
  file = {/home/peterprescott/Zotero/storage/ZVCTGM22/Zaharia et al. - 2012 - Resilient Distributed Datasets A Fault-Tolerant A.pdf},
  language = {en}
}

@misc{RGhadiyaram2016,
  title = {Difference between {{DataFrame}}, {{Dataset}}, and {{RDD}} in {{Spark}} ({{Answer}})},
  author = {Ghadiyaram, Ram},
  year = {2016},
  file = {/home/peterprescott/Zotero/storage/F7UZ9REC/difference-between-dataframe-dataset-and-rdd-in-spark.html},
  journal = {Stack Overflow}
}

@book{RKitchin2014,
  title = {The {{Data Revolution}}: {{Big Data}}, {{Open Data}}, {{Data Infrastructures}} and {{Their Consequences}}},
  shorttitle = {The {{Data Revolution}}},
  author = {Kitchin, Rob},
  year = {2014},
  month = aug,
  edition = {1 edition},
  publisher = {{SAGE Publications Ltd}},
  address = {{Los Angeles, California}},
  abstract = {"Carefully distinguishing between big data and open data, and exploring various data infrastructures, Kitchin vividly illustrates how the data landscape is rapidly changing and calls for a revolution in how we think about data." - Evelyn Ruppert, Goldsmiths, University of London  "Deconstructs the hype around the `data revolution' to carefully guide us through the histories and the futures of `big data.' The book skilfully engages with debates from across the humanities, social sciences, and sciences in order to produce a critical account of how data are enmeshed into enormous social, economic, and political changes that are taking place." - Mark Graham, University of Oxford  Traditionally, data has been a scarce commodity which, given its value, has been either jealously guarded or expensively traded.~ In recent years, technological developments and political lobbying have turned this position on its head. Data now flow as a deep and wide torrent, are low in cost and supported by robust infrastructures, and are increasingly open and accessible.~   A data revolution is underway, one that is already reshaping how knowledge is produced, business conducted, and governance enacted, as well as raising many questions concerning surveillance, privacy, security, profiling, social sorting, and intellectual property rights.~   In contrast to the hype and hubris of much media and business coverage, The Data Revolution provides a synoptic and critical analysis of the emerging data landscape.~ Accessible in style, the book provides:  A synoptic overview of big data, open data and data infrastructures An introduction to thinking conceptually about data, data infrastructures, data analytics and data markets Acritical discussion of the technical shortcomings and the social, political and ethical consequences of the data revolution An analysis of the implications of the data revolution to academic, business and government practices},
  isbn = {978-1-4462-8748-4},
  language = {English}
}


